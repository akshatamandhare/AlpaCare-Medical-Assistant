  0% 0/1125 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.
  9% 100/1125 [00:33<06:15,  2.73it/s]/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
{'loss': 3.2598, 'grad_norm': 0.7012163996696472, 'learning_rate': 0.0002, 'entropy': 3.492195963859558, 'num_tokens': 11136.0, 'mean_token_accuracy': 0.4000199377536774, 'epoch': 0.01}
{'loss': 3.088, 'grad_norm': 1.06106698513031, 'learning_rate': 0.0002, 'entropy': 3.0324996650218963, 'num_tokens': 23690.0, 'mean_token_accuracy': 0.4272958904504776, 'epoch': 0.02}
{'loss': 2.9658, 'grad_norm': 0.9362272024154663, 'learning_rate': 0.0002, 'entropy': 2.9794521749019625, 'num_tokens': 35768.0, 'mean_token_accuracy': 0.44560553058981894, 'epoch': 0.03}
{'loss': 2.784, 'grad_norm': 1.055793046951294, 'learning_rate': 0.0002, 'entropy': 2.8810725450515746, 'num_tokens': 48134.0, 'mean_token_accuracy': 0.4762948043644428, 'epoch': 0.04}
{'loss': 2.6501, 'grad_norm': 0.895455539226532, 'learning_rate': 0.0002, 'entropy': 2.6779381930828094, 'num_tokens': 60813.0, 'mean_token_accuracy': 0.47760315164923667, 'epoch': 0.04}
{'loss': 2.5846, 'grad_norm': 0.8705970644950867, 'learning_rate': 0.0002, 'entropy': 2.6425794541835783, 'num_tokens': 72769.0, 'mean_token_accuracy': 0.5035971105098724, 'epoch': 0.05}
{'loss': 2.5282, 'grad_norm': 0.8057345151901245, 'learning_rate': 0.0002, 'entropy': 2.5116856902837754, 'num_tokens': 85053.0, 'mean_token_accuracy': 0.5284764543175697, 'epoch': 0.06}
{'loss': 2.5066, 'grad_norm': 0.6925947666168213, 'learning_rate': 0.0002, 'entropy': 2.512962079048157, 'num_tokens': 97437.0, 'mean_token_accuracy': 0.5333264075219631, 'epoch': 0.07}
{'loss': 2.4091, 'grad_norm': 0.6948370337486267, 'learning_rate': 0.0002, 'entropy': 2.3564174622297287, 'num_tokens': 107962.0, 'mean_token_accuracy': 0.5568129725754261, 'epoch': 0.08}
{'loss': 2.3874, 'grad_norm': 0.6395414471626282, 'learning_rate': 0.0002, 'entropy': 2.419989049434662, 'num_tokens': 119683.0, 'mean_token_accuracy': 0.5500305287539959, 'epoch': 0.09}
  return fn(*args, **kwargs)
 18% 200/1125 [01:05<04:37,  3.34it/s]/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
{'loss': 2.3653, 'grad_norm': 0.8384767174720764, 'learning_rate': 0.0002, 'entropy': 2.368129399418831, 'num_tokens': 133745.0, 'mean_token_accuracy': 0.5409349516034127, 'epoch': 0.1}
{'loss': 2.4412, 'grad_norm': 0.6153974533081055, 'learning_rate': 0.0002, 'entropy': 2.41241292655468, 'num_tokens': 146465.0, 'mean_token_accuracy': 0.5417500957846642, 'epoch': 0.11}
{'loss': 2.3608, 'grad_norm': 1.1949654817581177, 'learning_rate': 0.0002, 'entropy': 2.3914640843868256, 'num_tokens': 158715.0, 'mean_token_accuracy': 0.5534927286207676, 'epoch': 0.12}
{'loss': 2.335, 'grad_norm': 0.842435359954834, 'learning_rate': 0.0002, 'entropy': 2.2901479721069338, 'num_tokens': 170935.0, 'mean_token_accuracy': 0.565578906238079, 'epoch': 0.12}
{'loss': 2.3702, 'grad_norm': 1.0169899463653564, 'learning_rate': 0.0002, 'entropy': 2.2963980615139006, 'num_tokens': 182475.0, 'mean_token_accuracy': 0.558092062920332, 'epoch': 0.13}
{'loss': 2.3134, 'grad_norm': 0.6883891820907593, 'learning_rate': 0.0002, 'entropy': 2.2935953736305237, 'num_tokens': 193862.0, 'mean_token_accuracy': 0.5710396125912667, 'epoch': 0.14}
{'loss': 2.3529, 'grad_norm': 0.682875394821167, 'learning_rate': 0.0002, 'entropy': 2.3649039268493652, 'num_tokens': 206480.0, 'mean_token_accuracy': 0.5544080108404159, 'epoch': 0.15}
{'loss': 2.2926, 'grad_norm': 0.7060060501098633, 'learning_rate': 0.0002, 'entropy': 2.2672233313322065, 'num_tokens': 218508.0, 'mean_token_accuracy': 0.5672825202345848, 'epoch': 0.16}
{'loss': 2.2345, 'grad_norm': 0.6292033195495605, 'learning_rate': 0.0002, 'entropy': 2.23548201918602, 'num_tokens': 230073.0, 'mean_token_accuracy': 0.5770381800830364, 'epoch': 0.17}
{'loss': 2.1835, 'grad_norm': 0.7655035257339478, 'learning_rate': 0.0002, 'entropy': 2.1643554717302322, 'num_tokens': 241870.0, 'mean_token_accuracy': 0.5877954676747322, 'epoch': 0.18}
  return fn(*args, **kwargs)
 27% 300/1125 [01:37<05:20,  2.58it/s]/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
{'loss': 2.2337, 'grad_norm': 0.8450377583503723, 'learning_rate': 0.0002, 'entropy': 2.2116593390703203, 'num_tokens': 253502.0, 'mean_token_accuracy': 0.5714397579431534, 'epoch': 0.19}
{'loss': 2.2353, 'grad_norm': 0.7993491291999817, 'learning_rate': 0.0002, 'entropy': 2.263794982433319, 'num_tokens': 265277.0, 'mean_token_accuracy': 0.5660168588161468, 'epoch': 0.2}
{'loss': 2.2251, 'grad_norm': 0.8802844882011414, 'learning_rate': 0.0002, 'entropy': 2.2126374334096908, 'num_tokens': 276834.0, 'mean_token_accuracy': 0.5794978097081185, 'epoch': 0.2}
{'loss': 2.2755, 'grad_norm': 0.6181321740150452, 'learning_rate': 0.0002, 'entropy': 2.2606630235910417, 'num_tokens': 288899.0, 'mean_token_accuracy': 0.5598665721714496, 'epoch': 0.21}
{'loss': 2.2494, 'grad_norm': 0.8493565917015076, 'learning_rate': 0.0002, 'entropy': 2.2106053292751313, 'num_tokens': 301354.0, 'mean_token_accuracy': 0.5736948855221271, 'epoch': 0.22}
{'loss': 2.1932, 'grad_norm': 0.6852580904960632, 'learning_rate': 0.0002, 'entropy': 2.1635202378034593, 'num_tokens': 312491.0, 'mean_token_accuracy': 0.584575767070055, 'epoch': 0.23}
{'loss': 2.1658, 'grad_norm': 0.6675062775611877, 'learning_rate': 0.0002, 'entropy': 2.143498879671097, 'num_tokens': 323296.0, 'mean_token_accuracy': 0.5937914319336415, 'epoch': 0.24}
{'loss': 2.3002, 'grad_norm': 0.7157478928565979, 'learning_rate': 0.0002, 'entropy': 2.190122205018997, 'num_tokens': 334107.0, 'mean_token_accuracy': 0.5794297307729721, 'epoch': 0.25}
{'loss': 2.2239, 'grad_norm': 0.5968857407569885, 'learning_rate': 0.0002, 'entropy': 2.198990139365196, 'num_tokens': 345273.0, 'mean_token_accuracy': 0.5775947958230973, 'epoch': 0.26}
{'loss': 2.1907, 'grad_norm': 0.5673931241035461, 'learning_rate': 0.0002, 'entropy': 2.1781111359596252, 'num_tokens': 356335.0, 'mean_token_accuracy': 0.5875895954668522, 'epoch': 0.27}
  return fn(*args, **kwargs)
 36% 400/1125 [02:09<03:35,  3.36it/s]/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
{'loss': 2.2527, 'grad_norm': 0.7634586691856384, 'learning_rate': 0.0002, 'entropy': 2.1924561321735383, 'num_tokens': 368587.0, 'mean_token_accuracy': 0.5736864224076271, 'epoch': 0.28}
{'loss': 2.2086, 'grad_norm': 0.8441930413246155, 'learning_rate': 0.0002, 'entropy': 2.161532324552536, 'num_tokens': 379866.0, 'mean_token_accuracy': 0.5861190497875214, 'epoch': 0.28}
{'loss': 2.2886, 'grad_norm': 0.635010302066803, 'learning_rate': 0.0002, 'entropy': 2.2443890273571014, 'num_tokens': 391427.0, 'mean_token_accuracy': 0.5815130718052387, 'epoch': 0.29}
{'loss': 2.187, 'grad_norm': 0.7522185444831848, 'learning_rate': 0.0002, 'entropy': 2.1511005848646163, 'num_tokens': 402514.0, 'mean_token_accuracy': 0.5834731385111809, 'epoch': 0.3}
{'loss': 2.3026, 'grad_norm': 0.7388182282447815, 'learning_rate': 0.0002, 'entropy': 2.2756264179944994, 'num_tokens': 415567.0, 'mean_token_accuracy': 0.5680827215313912, 'epoch': 0.31}
{'loss': 2.2422, 'grad_norm': 0.6991685032844543, 'learning_rate': 0.0002, 'entropy': 2.208411067724228, 'num_tokens': 427785.0, 'mean_token_accuracy': 0.5679401636123658, 'epoch': 0.32}
{'loss': 2.1084, 'grad_norm': 0.7222574353218079, 'learning_rate': 0.0002, 'entropy': 2.092021334171295, 'num_tokens': 438246.0, 'mean_token_accuracy': 0.5969241932034492, 'epoch': 0.33}
{'loss': 2.1759, 'grad_norm': 0.7998079061508179, 'learning_rate': 0.0002, 'entropy': 2.0969808638095855, 'num_tokens': 449696.0, 'mean_token_accuracy': 0.5848959021270275, 'epoch': 0.34}
{'loss': 2.1825, 'grad_norm': 0.5476580858230591, 'learning_rate': 0.0002, 'entropy': 2.153905412554741, 'num_tokens': 461074.0, 'mean_token_accuracy': 0.5932472713291645, 'epoch': 0.35}
{'loss': 2.1835, 'grad_norm': 0.7434984445571899, 'learning_rate': 0.0002, 'entropy': 2.2016751050949095, 'num_tokens': 473417.0, 'mean_token_accuracy': 0.5781561255455017, 'epoch': 0.36}
  return fn(*args, **kwargs)
 44% 500/1125 [02:41<03:31,  2.95it/s]/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
{'loss': 2.1935, 'grad_norm': 0.7855135202407837, 'learning_rate': 0.0002, 'entropy': 2.1208010792732237, 'num_tokens': 483947.0, 'mean_token_accuracy': 0.5819393113255501, 'epoch': 0.36}
{'loss': 2.2808, 'grad_norm': 0.6635628938674927, 'learning_rate': 0.0002, 'entropy': 2.2858239650726317, 'num_tokens': 496524.0, 'mean_token_accuracy': 0.5619784913957119, 'epoch': 0.37}
{'loss': 2.1861, 'grad_norm': 0.6789058446884155, 'learning_rate': 0.0002, 'entropy': 2.183161136507988, 'num_tokens': 508563.0, 'mean_token_accuracy': 0.5747240222990513, 'epoch': 0.38}
{'loss': 2.2006, 'grad_norm': 0.6350353360176086, 'learning_rate': 0.0002, 'entropy': 2.2016289830207825, 'num_tokens': 521438.0, 'mean_token_accuracy': 0.5606181204319001, 'epoch': 0.39}
{'loss': 2.2346, 'grad_norm': 0.6011949777603149, 'learning_rate': 0.0002, 'entropy': 2.1706286907196044, 'num_tokens': 532933.0, 'mean_token_accuracy': 0.5768122181296349, 'epoch': 0.4}
{'loss': 2.2198, 'grad_norm': 0.7087604999542236, 'learning_rate': 0.0002, 'entropy': 2.2117056101560593, 'num_tokens': 545259.0, 'mean_token_accuracy': 0.5751934066414833, 'epoch': 0.41}
{'loss': 2.1971, 'grad_norm': 0.7201566696166992, 'learning_rate': 0.0002, 'entropy': 2.2088233649730684, 'num_tokens': 557594.0, 'mean_token_accuracy': 0.5789595663547515, 'epoch': 0.42}
{'loss': 2.1107, 'grad_norm': 0.5796128511428833, 'learning_rate': 0.0002, 'entropy': 2.0677020102739334, 'num_tokens': 569043.0, 'mean_token_accuracy': 0.5890189863741397, 'epoch': 0.43}
{'loss': 2.1638, 'grad_norm': 0.8308295011520386, 'learning_rate': 0.0002, 'entropy': 2.0940476059913635, 'num_tokens': 580851.0, 'mean_token_accuracy': 0.5825511112809181, 'epoch': 0.44}
{'loss': 2.1349, 'grad_norm': 0.5937808752059937, 'learning_rate': 0.0002, 'entropy': 2.140808975696564, 'num_tokens': 591335.0, 'mean_token_accuracy': 0.5908084623515606, 'epoch': 0.44}
  return fn(*args, **kwargs)
 53% 600/1125 [03:12<02:35,  3.37it/s]/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
{'loss': 2.1912, 'grad_norm': 0.7039317488670349, 'learning_rate': 0.0002, 'entropy': 2.1600149244070055, 'num_tokens': 602596.0, 'mean_token_accuracy': 0.5862237550318241, 'epoch': 0.45}
{'loss': 2.2285, 'grad_norm': 0.7581743597984314, 'learning_rate': 0.0002, 'entropy': 2.1483227878808977, 'num_tokens': 614683.0, 'mean_token_accuracy': 0.5807108126580716, 'epoch': 0.46}
{'loss': 2.2406, 'grad_norm': 0.7268393039703369, 'learning_rate': 0.0002, 'entropy': 2.195618298649788, 'num_tokens': 626822.0, 'mean_token_accuracy': 0.5777264468371868, 'epoch': 0.47}
{'loss': 2.1579, 'grad_norm': 0.743478536605835, 'learning_rate': 0.0002, 'entropy': 2.133622807264328, 'num_tokens': 639438.0, 'mean_token_accuracy': 0.5843034088611603, 'epoch': 0.48}
{'loss': 2.168, 'grad_norm': 0.7617989778518677, 'learning_rate': 0.0002, 'entropy': 2.096354764699936, 'num_tokens': 650426.0, 'mean_token_accuracy': 0.5979965858161449, 'epoch': 0.49}
{'loss': 2.1692, 'grad_norm': 0.5907584428787231, 'learning_rate': 0.0002, 'entropy': 2.139511677622795, 'num_tokens': 661724.0, 'mean_token_accuracy': 0.5823730386793613, 'epoch': 0.5}
{'loss': 2.1092, 'grad_norm': 0.6787155866622925, 'learning_rate': 0.0002, 'entropy': 2.1110061019659043, 'num_tokens': 673833.0, 'mean_token_accuracy': 0.5867521055042744, 'epoch': 0.51}
{'loss': 2.0329, 'grad_norm': 0.7938295602798462, 'learning_rate': 0.0002, 'entropy': 2.0315110713243483, 'num_tokens': 684114.0, 'mean_token_accuracy': 0.5976238697767258, 'epoch': 0.52}
{'loss': 2.1486, 'grad_norm': 0.7506057620048523, 'learning_rate': 0.0002, 'entropy': 2.125340473651886, 'num_tokens': 696070.0, 'mean_token_accuracy': 0.5836602203547955, 'epoch': 0.52}
{'loss': 2.0886, 'grad_norm': 0.6486045122146606, 'learning_rate': 0.0002, 'entropy': 2.055138701200485, 'num_tokens': 707560.0, 'mean_token_accuracy': 0.5971939615905285, 'epoch': 0.53}
  return fn(*args, **kwargs)
 62% 700/1125 [03:44<02:07,  3.33it/s]/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
{'loss': 2.1621, 'grad_norm': 0.6436284780502319, 'learning_rate': 0.0002, 'entropy': 2.1446093946695326, 'num_tokens': 719047.0, 'mean_token_accuracy': 0.5766410268843174, 'epoch': 0.54}
{'loss': 2.2447, 'grad_norm': 0.6141039729118347, 'learning_rate': 0.0002, 'entropy': 2.2225585728883743, 'num_tokens': 731349.0, 'mean_token_accuracy': 0.5681721158325672, 'epoch': 0.55}
{'loss': 2.2344, 'grad_norm': 0.6365211606025696, 'learning_rate': 0.0002, 'entropy': 2.15781164765358, 'num_tokens': 743205.0, 'mean_token_accuracy': 0.5762567654252052, 'epoch': 0.56}
{'loss': 2.1823, 'grad_norm': 0.6843046545982361, 'learning_rate': 0.0002, 'entropy': 2.142120423913002, 'num_tokens': 755524.0, 'mean_token_accuracy': 0.5756133332848549, 'epoch': 0.57}
{'loss': 2.2151, 'grad_norm': 0.7082284092903137, 'learning_rate': 0.0002, 'entropy': 2.165374740958214, 'num_tokens': 767562.0, 'mean_token_accuracy': 0.5767703868448735, 'epoch': 0.58}
{'loss': 2.1942, 'grad_norm': 0.6493350267410278, 'learning_rate': 0.0002, 'entropy': 2.16519156396389, 'num_tokens': 779094.0, 'mean_token_accuracy': 0.5788943290710449, 'epoch': 0.59}
{'loss': 2.1963, 'grad_norm': 0.6271071434020996, 'learning_rate': 0.0002, 'entropy': 2.177992731332779, 'num_tokens': 790838.0, 'mean_token_accuracy': 0.5716038629412651, 'epoch': 0.6}
{'loss': 2.1628, 'grad_norm': 0.6702864766120911, 'learning_rate': 0.0002, 'entropy': 2.12878859937191, 'num_tokens': 801661.0, 'mean_token_accuracy': 0.5803596951067448, 'epoch': 0.6}
{'loss': 2.1232, 'grad_norm': 0.6282434463500977, 'learning_rate': 0.0002, 'entropy': 2.0987206220626833, 'num_tokens': 813022.0, 'mean_token_accuracy': 0.5901525661349296, 'epoch': 0.61}
{'loss': 2.1306, 'grad_norm': 0.7667525410652161, 'learning_rate': 0.0002, 'entropy': 2.090930873155594, 'num_tokens': 825267.0, 'mean_token_accuracy': 0.5991695716977119, 'epoch': 0.62}
  return fn(*args, **kwargs)
 71% 800/1125 [04:16<01:40,  3.25it/s]/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
{'loss': 2.1321, 'grad_norm': 0.5901862382888794, 'learning_rate': 0.0002, 'entropy': 2.0783100336790086, 'num_tokens': 836909.0, 'mean_token_accuracy': 0.5913365684449673, 'epoch': 0.63}
{'loss': 2.1795, 'grad_norm': 0.7515323162078857, 'learning_rate': 0.0002, 'entropy': 2.163903456926346, 'num_tokens': 849221.0, 'mean_token_accuracy': 0.5734854273498058, 'epoch': 0.64}
{'loss': 2.1498, 'grad_norm': 0.6465572118759155, 'learning_rate': 0.0002, 'entropy': 2.1704303294420244, 'num_tokens': 861085.0, 'mean_token_accuracy': 0.5777743920683861, 'epoch': 0.65}
{'loss': 2.2113, 'grad_norm': 0.696410059928894, 'learning_rate': 0.0002, 'entropy': 2.173758637905121, 'num_tokens': 872794.0, 'mean_token_accuracy': 0.5777109511196613, 'epoch': 0.66}
{'loss': 2.174, 'grad_norm': 0.7582695484161377, 'learning_rate': 0.0002, 'entropy': 2.107764744758606, 'num_tokens': 884476.0, 'mean_token_accuracy': 0.5883249789476395, 'epoch': 0.67}
{'loss': 2.2218, 'grad_norm': 0.6591717600822449, 'learning_rate': 0.0002, 'entropy': 2.1400375187397005, 'num_tokens': 896842.0, 'mean_token_accuracy': 0.5753662586212158, 'epoch': 0.68}
{'loss': 2.088, 'grad_norm': 0.6516067981719971, 'learning_rate': 0.0002, 'entropy': 2.1018853932619095, 'num_tokens': 908014.0, 'mean_token_accuracy': 0.5934830427169799, 'epoch': 0.68}
{'loss': 2.1401, 'grad_norm': 0.7321612238883972, 'learning_rate': 0.0002, 'entropy': 2.092611438035965, 'num_tokens': 920009.0, 'mean_token_accuracy': 0.5948645412921906, 'epoch': 0.69}
{'loss': 2.2185, 'grad_norm': 0.6983242630958557, 'learning_rate': 0.0002, 'entropy': 2.1568107545375823, 'num_tokens': 932171.0, 'mean_token_accuracy': 0.5770164363086223, 'epoch': 0.7}
{'loss': 2.1393, 'grad_norm': 0.5818511843681335, 'learning_rate': 0.0002, 'entropy': 2.1455868005752565, 'num_tokens': 944335.0, 'mean_token_accuracy': 0.5813306242227554, 'epoch': 0.71}
  return fn(*args, **kwargs)
 80% 900/1125 [04:47<01:06,  3.39it/s]/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
{'loss': 2.1908, 'grad_norm': 0.5996249914169312, 'learning_rate': 0.0002, 'entropy': 2.1837458550930022, 'num_tokens': 957781.0, 'mean_token_accuracy': 0.5739461645483971, 'epoch': 0.72}
{'loss': 2.134, 'grad_norm': 0.7760807871818542, 'learning_rate': 0.0002, 'entropy': 2.1199982076883317, 'num_tokens': 970463.0, 'mean_token_accuracy': 0.5818493947386741, 'epoch': 0.73}
{'loss': 2.1427, 'grad_norm': 0.7090671062469482, 'learning_rate': 0.0002, 'entropy': 2.103209602832794, 'num_tokens': 981768.0, 'mean_token_accuracy': 0.5889890007674694, 'epoch': 0.74}
{'loss': 2.1018, 'grad_norm': 0.6443836092948914, 'learning_rate': 0.0002, 'entropy': 2.068253034353256, 'num_tokens': 993419.0, 'mean_token_accuracy': 0.583836267888546, 'epoch': 0.75}
{'loss': 2.1597, 'grad_norm': 0.7515588402748108, 'learning_rate': 0.0002, 'entropy': 2.1435991138219834, 'num_tokens': 1005868.0, 'mean_token_accuracy': 0.577349716424942, 'epoch': 0.76}
{'loss': 2.1744, 'grad_norm': 0.5846339464187622, 'learning_rate': 0.0002, 'entropy': 2.1516885817050935, 'num_tokens': 1017644.0, 'mean_token_accuracy': 0.5702464744448662, 'epoch': 0.76}
{'loss': 2.1016, 'grad_norm': 0.6060044765472412, 'learning_rate': 0.0002, 'entropy': 2.076010173559189, 'num_tokens': 1028844.0, 'mean_token_accuracy': 0.5998656637966633, 'epoch': 0.77}
{'loss': 2.141, 'grad_norm': 0.6503177881240845, 'learning_rate': 0.0002, 'entropy': 2.1163462191820144, 'num_tokens': 1041291.0, 'mean_token_accuracy': 0.5833192400634288, 'epoch': 0.78}
{'loss': 2.1594, 'grad_norm': 0.7146230936050415, 'learning_rate': 0.0002, 'entropy': 2.119324120879173, 'num_tokens': 1053331.0, 'mean_token_accuracy': 0.5829486742615699, 'epoch': 0.79}
{'loss': 2.0746, 'grad_norm': 0.9029910564422607, 'learning_rate': 0.0002, 'entropy': 2.0056782126426698, 'num_tokens': 1064163.0, 'mean_token_accuracy': 0.6082249514758586, 'epoch': 0.8}
  return fn(*args, **kwargs)
 89% 1000/1125 [05:19<00:38,  3.28it/s]/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
{'loss': 2.1404, 'grad_norm': 0.6914989948272705, 'learning_rate': 0.0002, 'entropy': 2.1338338643312453, 'num_tokens': 1076271.0, 'mean_token_accuracy': 0.5794271990656853, 'epoch': 0.81}
{'loss': 2.1138, 'grad_norm': 0.7446209788322449, 'learning_rate': 0.0002, 'entropy': 2.0733310505747795, 'num_tokens': 1087929.0, 'mean_token_accuracy': 0.5886604107916356, 'epoch': 0.82}
{'loss': 2.1519, 'grad_norm': 0.5653941035270691, 'learning_rate': 0.0002, 'entropy': 2.1093707889318467, 'num_tokens': 1100329.0, 'mean_token_accuracy': 0.5861158318817615, 'epoch': 0.83}
{'loss': 2.1044, 'grad_norm': 0.6944187879562378, 'learning_rate': 0.0002, 'entropy': 2.0594657361507416, 'num_tokens': 1111847.0, 'mean_token_accuracy': 0.5930916227400302, 'epoch': 0.84}
{'loss': 2.1049, 'grad_norm': 0.8145411610603333, 'learning_rate': 0.0002, 'entropy': 2.1080225139856337, 'num_tokens': 1124454.0, 'mean_token_accuracy': 0.5816387422382832, 'epoch': 0.84}
{'loss': 2.1626, 'grad_norm': 0.7072836756706238, 'learning_rate': 0.0002, 'entropy': 2.1265364229679107, 'num_tokens': 1137316.0, 'mean_token_accuracy': 0.5708514250814914, 'epoch': 0.85}
{'loss': 2.081, 'grad_norm': 0.7661979794502258, 'learning_rate': 0.0002, 'entropy': 2.0121287286281584, 'num_tokens': 1148309.0, 'mean_token_accuracy': 0.5999741412699222, 'epoch': 0.86}
{'loss': 2.1309, 'grad_norm': 0.6382297277450562, 'learning_rate': 0.0002, 'entropy': 2.106538248062134, 'num_tokens': 1159966.0, 'mean_token_accuracy': 0.5851149402558804, 'epoch': 0.87}
{'loss': 2.1537, 'grad_norm': 0.6468756794929504, 'learning_rate': 0.0002, 'entropy': 2.1701058238744735, 'num_tokens': 1172191.0, 'mean_token_accuracy': 0.5786122210323811, 'epoch': 0.88}
{'loss': 2.1497, 'grad_norm': 0.75029456615448, 'learning_rate': 0.0002, 'entropy': 2.0962642788887025, 'num_tokens': 1184598.0, 'mean_token_accuracy': 0.5783939719200134, 'epoch': 0.89}
  return fn(*args, **kwargs)
 98% 1100/1125 [05:51<00:07,  3.34it/s]/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
{'loss': 2.1394, 'grad_norm': 0.6971508860588074, 'learning_rate': 0.0002, 'entropy': 2.136040371656418, 'num_tokens': 1196397.0, 'mean_token_accuracy': 0.5784227527678013, 'epoch': 0.9}
{'loss': 2.1862, 'grad_norm': 0.7807457447052002, 'learning_rate': 0.0002, 'entropy': 2.0970632672309875, 'num_tokens': 1208549.0, 'mean_token_accuracy': 0.5861597418785095, 'epoch': 0.91}
{'loss': 2.1581, 'grad_norm': 0.8154586553573608, 'learning_rate': 0.0002, 'entropy': 2.107338288426399, 'num_tokens': 1219488.0, 'mean_token_accuracy': 0.5936746321618557, 'epoch': 0.92}
{'loss': 2.1157, 'grad_norm': 0.6342622637748718, 'learning_rate': 0.0002, 'entropy': 2.067728638648987, 'num_tokens': 1232206.0, 'mean_token_accuracy': 0.5885651335120201, 'epoch': 0.92}
{'loss': 2.0603, 'grad_norm': 0.8479377627372742, 'learning_rate': 0.0002, 'entropy': 2.0332363158464433, 'num_tokens': 1243875.0, 'mean_token_accuracy': 0.5890735246241092, 'epoch': 0.93}
{'loss': 2.1836, 'grad_norm': 0.6727744340896606, 'learning_rate': 0.0002, 'entropy': 2.128613346815109, 'num_tokens': 1257152.0, 'mean_token_accuracy': 0.5769610501825809, 'epoch': 0.94}
{'loss': 2.089, 'grad_norm': 0.7675089836120605, 'learning_rate': 0.0002, 'entropy': 2.055611398816109, 'num_tokens': 1268547.0, 'mean_token_accuracy': 0.601260992884636, 'epoch': 0.95}
{'loss': 2.0007, 'grad_norm': 0.6631559729576111, 'learning_rate': 0.0002, 'entropy': 1.9634651958942413, 'num_tokens': 1279287.0, 'mean_token_accuracy': 0.6148454055190087, 'epoch': 0.96}
{'loss': 2.1919, 'grad_norm': 0.5814210176467896, 'learning_rate': 0.0002, 'entropy': 2.1647716015577316, 'num_tokens': 1292835.0, 'mean_token_accuracy': 0.5716369263827801, 'epoch': 0.97}
{'loss': 2.0984, 'grad_norm': 0.6384245753288269, 'learning_rate': 0.0002, 'entropy': 2.119772145152092, 'num_tokens': 1305314.0, 'mean_token_accuracy': 0.5820361509919166, 'epoch': 0.98}
  return fn(*args, **kwargs)
100% 1125/1125 [06:00<00:00,  3.12it/s]
{'loss': 2.0029, 'grad_norm': 0.6479055881500244, 'learning_rate': 0.0002, 'entropy': 1.9709706485271454, 'num_tokens': 1315574.0, 'mean_token_accuracy': 0.6024729110300541, 'epoch': 0.99}
{'loss': 2.1194, 'grad_norm': 0.6417790651321411, 'learning_rate': 0.0002, 'entropy': 2.1003067791461945, 'num_tokens': 1326877.0, 'mean_token_accuracy': 0.5896897189319134, 'epoch': 1.0}
{'train_runtime': 390.3818, 'train_samples_per_second': 11.527, 'train_steps_per_second': 2.882, 'train_loss': 2.2293597547743054, 'entropy': 2.044132936000824, 'num_tokens': 1332911.0, 'mean_token_accuracy': 0.5938549280166626, 'epoch': 1.0}

✅ Training completed in: 0:06:30.957521
Saving model to: ./alpacare-lora-adapter
✅ Training information saved!

============================================================
🎉 TRAINING COMPLETED SUCCESSFULLY!
============================================================
✅ LoRA adapter saved to: ./alpacare-lora-adapter
✅ You can now use the inference script or notebook
============================================================
